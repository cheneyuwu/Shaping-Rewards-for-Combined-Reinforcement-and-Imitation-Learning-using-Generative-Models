Logging to /home/yuchen/Desktop/FlorianResearch/RLProject/temp/log

*** ddpg_params ***
Q_lr: 0.01
T: 200
action_l2: 1.0
batch_size: 256
buffer_size: 1000000
clip_obs: 200.0
clip_pos_returns: True
clip_return: 199.99999999999983
gamma: 0.995
hidden: 64
info: {'env_name': 'Pendulum-v0'}
input_dims: {'o': 3, 'u': 1}
layers: 2
max_u: 2.0
network_class: yw.ddpg_no_goal.actor_critic:ActorCritic
norm_clip: 5
norm_eps: 0.01
pi_lr: 0.001
polyak: 0.95
relative_goals: False
rollout_batch_size: 1
sample_transitions: <function make_sample_transitions.<locals>._sample_transitions at 0x7f2b73d69e18>
scope: ddpg
subtract_goals: <function simple_goal_subtract at 0x7f2b73d66e18>
Preparing staging area for feeding data to the model.
Creating a DDPG agent with action space 1 x 2.0...
Configure the replay buffer.

*** rollout_params ***
T: 200
compute_Q: False
dims: {'o': 3, 'u': 1}
exploit: False
noise_eps: 0.2
random_eps: 0.3
rollout_batch_size: 1
use_demo_states: True
use_target_net: False

*** eval_params ***
T: 200
compute_Q: True
dims: {'o': 3, 'u': 1}
exploit: True
noise_eps: 0.2
random_eps: 0.3
rollout_batch_size: 1
use_demo_states: False
use_target_net: False

*** demo_params ***
T: 200
compute_Q: False
dims: {'o': 3, 'u': 1}
exploit: True
noise_eps: 0.2
random_eps: 0.3
render: True
rollout_batch_size: 1
use_demo_states: False
use_target_net: True

*** params ***
T: 200
_Q_lr: 0.01
_action_l2: 1.0
_batch_size: 256
_buffer_size: 1000000
_clip_obs: 200.0
_hidden: 64
_layers: 2
_max_u: 2.0
_network_class: yw.ddpg_no_goal.actor_critic:ActorCritic
_norm_clip: 5
_norm_eps: 0.01
_pi_lr: 0.001
_polyak: 0.95
_relative_goals: False
_scope: ddpg
clip_return: 1
dims: {'o': 3, 'u': 1}
env_name: Pendulum-v0
gamma: 0.995
make_env: <function prepare_params.<locals>.make_env at 0x7f2b73d69d90>
n_batches: 40
n_cycles: 10
n_test_rollouts: 10
noise_eps: 0.2
random_eps: 0.3
rank_seed: 4000000
replay_k: 4
replay_strategy: none
rollout_batch_size: 1
test_with_polyak: False

*** Training ***
