Logging to ../temp/log/

*** her_params ***
replay_k: 4
replay_strategy: future
reward_fun: <function configure_her.<locals>.reward_fun at 0x7fdcce3f6268>

*** ddpg_params ***
Q_lr: 0.001
T: 50
action_l2: 1.0
batch_size: 256
buffer_size: 1000000
clip_obs: 200.0
clip_pos_returns: True
clip_return: 49.99999999999996
gamma: 0.98
hidden: 256
info: {'env_name': 'FetchReachDense-v1'}
input_dims: {'o': 10, 'u': 4, 'g': 3, 'info_is_success': 1}
layers: 3
max_u: 1.0
network_class: yw.ddpg.actor_critic:ActorCritic
norm_clip: 5
norm_eps: 0.01
pi_lr: 0.001
polyak: 0.95
relative_goals: False
rollout_batch_size: 5
sample_transitions: <function make_sample_her_transitions.<locals>._sample_her_transitions at 0x7fdcce3f62f0>
scope: ddpg
subtract_goals: <function simple_goal_subtract at 0x7fdcced3ad08>
Preparing staging area for feeding data to the model.
Creating a DDPG agent with action space 4 x 1.0...
